\section{Technical Implementation}
\label{sec:implementation}

\subsection{Technology Stack}

The implementation leverages a carefully selected technology stack optimized for AI, data processing, and web deployment. The core framework is datapizza-ai,\footnote{\url{https://github.com/datapizza-labs/datapizza-ai}} a modern italian AI framework specifically designed for building conversational agents with complex orchestration requirements. The datapizza-ai framework provides comprehensive abstractions for agent lifecycle management, message routing, and tool invocation. It abstracts away the complexities of different LLM provider APIs and authentication mechanisms, allowing the system to seamlessly switch between Ollama for local inference, Google Gemini for cloud-based capabilities, or OpenAI's models, without requiring changes to the agent implementation. The framework manages conversation context and memory through a unified interface, handling both immediate conversation history for contextual understanding and persistent memory for long-term user profile management. Additionally, datapizza-ai provides sophisticated tool management capabilities that enable agents to register and invoke external functions as part of their decision-making process, facilitating integration with specialized services like the RAG retriever.

The web framework is Streamlit, a Python-based framework enabling rapid development of data applications without requiring JavaScript expertise. Streamlit handles reactive UI updates and state management automatically, greatly reducing development overhead. Pydantic provides runtime type validation and serialization for data models, used extensively for FinancialProfile, Portfolio, and related models, ensuring data consistency throughout the system. Qdrant serves as a vector database engine for semantic search over financial documents and embeddings. Plotly enables interactive, publication-quality financial visualizations including candlestick charts, return distributions, and allocation pie charts.

The data processing layer relies on Pandas for tabular data manipulation and financial time-series analysis, NumPy for numerical computations and matrix operations, and scikit-learn for statistical analysis and machine learning utilities.

The infrastructure components include Docker for container orchestration, Docker Compose for multi-container application management, and Ollama as a local LLM runtime environment. This comprehensive technology stack provides all necessary capabilities for building a production-quality financial AI system.

\subsection{Agent Implementation}

The agent architecture is built on the datapizza-ai framework, which provides a flexible agent abstraction layer for building intelligent conversational systems. The architecture follows a layered approach where base agent functionality is defined at the foundational level, handling common concerns like configuration management, initialization of language model clients based on selected providers, loading and customization of system prompts, and registration of available tools. This foundation allows specialized agents to be developed by extending base functionality with domain-specific behavior.

Two specialized agent implementations extend this foundation. The ChatbotAgent provides a pure conversational interface focused on engaging users and gathering information without invoking specialized financial tools. It maintains a sequence of strategically designed questions that are presented progressively throughout the conversation, allowing the system to collect financial preferences, risk profiles, and investment objectives in a natural dialogue flow. The chatbot combines language model responses with guided question management, creating a conversational experience that both answers user queries and systematically gathers the information needed for financial profiling.

Financial-domain agents extend the foundation with specialized capabilities tailored to wealth management and investment advising. These agents are configured with financial-specific system prompts that guide the language model toward appropriate financial reasoning. The agents are trained to recognize when they need to invoke specialized tools, such as structured data extraction from conversation history into financial profiles, synthesis of portfolio recommendations based on user constraints and market data, computation of advanced metrics and performance analytics, and construction of semantic queries for the retrieval-augmented generation system. The agents leverage the language model's capability to produce structured outputs, enabling reliable extraction of complex financial information from unstructured conversations.

A key characteristic of the agent implementation is its ability to manage multi-turn conversations where information is progressively gathered and refined. Through datapizza-ai's memory management capabilities, agents maintain awareness of previous interactions, allowing for contextual understanding and smoother user experiences. The agents orchestrate complex workflows that combine language model reasoning with tool invocation, vector similarity search, and financial calculations in a seamless manner.

\subsection{Data Model Validation}

Pydantic models ensure data integrity throughout the system. FinancialProfile enforces type safety for all financial attributes, provides default values for missing information, includes field descriptions for API documentation, and supports optional fields for incomplete data. Similar validation is applied to Portfolio and other data structures. This approach catches data errors early and provides clear error messages when data does not conform to expected schemas.

\subsection{Testing Infrastructure}

The project includes a comprehensive test suite organized by category. Unit tests examine individual components in isolation, including financial profile extraction, portfolio analysis, and asset retrieval. Integration tests verify complete workflows such as RAG retrieval and portfolio generation. Fixture-based testing provides shared test data and configurations across tests. Tests are executed using pytest with the ability to run specific test categories or generate coverage reports.

\subsection{DevOps and Continuous Integration}

The project implements a comprehensive continuous integration and continuous deployment pipeline leveraging GitHub Actions to ensure code quality and reliable releases. The automated testing pipeline executes the full test suite on every push to main branches and on pull requests, validating that changes do not introduce regressions. The test environment is configured with Python 3.11 to match production requirements, and critical API keys are provided through secure secrets management to enable testing of cloud-based LLM providers.

Code quality enforcement is implemented through multiple mechanisms. Conventional commits validation ensures that all commit messages follow established standards for clarity and automated changelog generation, improving project history readability and enabling semantic versioning. Pre-commit hooks are automatically executed before commits are recorded, running code formatters and linters to maintain consistent style. The black code formatter ensures uniform code formatting throughout the project, while import sorting tools organize dependencies in a canonical order. These automated checks prevent many common code quality issues from entering the codebase.

Docker image validation is performed on pull requests and pushes to main branches, testing that the containerized application builds successfully and runs without errors. This early detection of deployment issues prevents broken container images from being pushed to registries. When code is merged to the main branch and all validation checks pass, a semantic release process automatically analyzes commit messages to determine version numbers and generate release notes, maintaining a changelog without manual intervention. Upon successful semantic release, the Docker image is automatically built with the new version tag and pushed to Docker Hub, making the latest build immediately available for deployment.

The infrastructure as code approach, enabled through Docker Compose configuration files and environment-based configuration, allows reproducible deployments across different environments. The combination of automated testing, code quality enforcement, and container validation creates a robust pipeline that catches issues early while enabling rapid iteration and reliable production deployments.

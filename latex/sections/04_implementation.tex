\section{Technical Implementation}
\label{sec:implementation}

\subsection{Technology Stack}

The implementation leverages a carefully selected technology stack optimized for AI, data processing, and web deployment. The core framework is datapizza-ai, a modern Italian AI framework specifically designed for building conversational agents with complex orchestration requirements. The datapizza-ai framework provides comprehensive abstractions for agent lifecycle management, message routing, and tool invocation. It abstracts away the complexities of different LLM provider APIs and authentication mechanisms, allowing the system to seamlessly switch between Ollama for local inference, Google Gemini for cloud-based capabilities, or OpenAI's models, without requiring changes to the agent implementation. The framework manages conversation context and memory through a unified interface, handling both immediate conversation history for contextual understanding and persistent memory for long-term user profile management. Additionally, datapizza-ai provides sophisticated tool management capabilities that enable agents to register and invoke external functions as part of their decision-making process, facilitating integration with specialized services like the RAG retriever.

The web framework is Streamlit, a Python-based framework enabling rapid development of data applications without requiring JavaScript expertise. Streamlit handles reactive UI updates and state management automatically, greatly reducing development overhead. Pydantic provides runtime type validation and serialization for data models, used extensively for Financial Profile, Portfolio, and related models, ensuring data consistency throughout the system. Qdrant serves as a vector database engine for semantic search over financial documents and embeddings. Plotly enables interactive, publication-quality financial visualizations including candlestick charts, return distributions, and allocation pie charts.

The data processing layer relies on Pandas for tabular data manipulation and financial time-series analysis, NumPy for numerical computations and matrix operations, and scikit-learn for statistical analysis and machine learning utilities.

The infrastructure components include Docker for container orchestration, Docker Compose for multi-container application management, and Ollama as a local LLM runtime environment. This comprehensive technology stack provides all necessary capabilities for building a production-quality financial AI system.

\subsection{Agent Implementation}

The agent architecture, detailed in Section~\ref{sec:architecture}, is built on the datapizza-ai framework and implements a layered approach with base agent functionality and specialized implementations for both conversational and financial-domain interactions. Agents manage multi-turn conversations with memory management capabilities and orchestrate complex workflows combining language model reasoning with tool invocation and financial calculations.

\subsection{Data Model Validation}

Pydantic models ensure data integrity throughout the system. Financial Profile enforces type safety for all financial attributes, provides default values for missing information, includes field descriptions for API documentation, and supports optional fields for incomplete data. Similar validation is applied to Portfolio and other data structures. This approach catches data errors early and provides clear error messages when data does not conform to expected schemas.

\subsection{DevOps and Continuous Integration}

The project implements a comprehensive continuous integration and continuous deployment pipeline leveraging GitHub Actions to ensure code quality and reliable releases. The automated testing pipeline executes the full test suite on every push to main branches and on pull requests, validating that changes do not introduce regressions. The test environment is configured with Python 3.11 to match production requirements, and critical API keys are provided through secure secrets management to enable testing of cloud-based LLM providers.

Code quality enforcement is implemented through multiple mechanisms. Conventional commits validation ensures that all commit messages follow established standards for clarity and automated changelog generation, improving project history readability and enabling semantic versioning. Pre-commit hooks are automatically executed before commits are recorded, running code formatters and linters to maintain consistent style. The black code formatter ensures uniform code formatting throughout the project, while import sorting tools organize dependencies in a canonical order. These automated checks prevent many common code quality issues from entering the codebase.

Docker image validation is performed on pull requests and pushes to main branches, testing that the containerized application builds successfully and runs without errors. This early detection of deployment issues prevents broken container images from being pushed to registries. When code is merged to the main branch and all validation checks pass, a semantic release process automatically analyzes commit messages to determine version numbers and generate release notes, maintaining a changelog without manual intervention. Upon successful semantic release, the Docker image is automatically built with the new version tag and pushed to Docker Hub, making the latest build immediately available for deployment.

The infrastructure as code approach, enabled through Docker Compose configuration files and environment-based configuration, allows reproducible deployments across different environments. The combination of automated testing, code quality enforcement, and container validation creates a robust pipeline that catches issues early while enabling rapid iteration and reliable production deployments.
